![네트워크 구조](https://mblogthumb-phinf.pstatic.net/MjAxNzA1MjRfMTUx/MDAxNDk1NTc3Nzc0Njkx.8puVyiN2JxwRfRHXTqfADa756daU1Lot-4KRIJf3iEgg.qxH477ZyCiwJBznmgKlnpVrrDGqezGVcHhvQH5-hBwUg.PNG.sogangori/multitask_cascase_vs_joint_learning.png?type=w2)

![Object Detection](https://mblogthumb-phinf.pstatic.net/MjAxNzA2MTVfMjc4/MDAxNDk3NDg2MTMxMzY5.x_zQ0lOYXXqSWaE1Iqv3L_DopMyoj8JlBwsWH7HgSpEg.6Mg2nntY74nlGOJmEzUOxONTUf_TRlKPoKfLr9tipvQg.PNG.sogangori/localizationVsDetection.png?type=w2)

# CNN (Convolutional Neural Network)

패딩

왜 (제로)패딩을 사용할까? 앞에서 본 것과 같이 CONV 레이어를 통과하면서 spatial 크기를 그대로 유지하게 해준다는 점 외에도, 패딩을 쓰면 성능도 향상된다. 만약 제로 패딩을 하지 않고 valid convolution (패딩을 하지 않은 convolution)을 한다면 볼륨의 크기는 CONV 레이어를 거칠 때마다 줄어들게 되고, 가장자리의 정보들이 빠르게 사라진다.



***


# R-CNN (Region CNN)

간단히 말하자면, R-CNN은 Input image에서 수많은 Object 후보들을 찾아내고(Selective Search) 이들을 모두 CNN에 넣어서 Feature를 뽑아낸다. 그리고 뽑아낸 Feature들을 SVM등의 Classifier에 넣어서 분류작업을 수행하고 NMS와 같은 기법으로 Bounding Box를 이미지 위에 그려내는 방법이다. 다음 그림은 전체적인 과정에 대한 이해를 도와준다.

R-CNN의 Training을 느리게 만드는 가장 주된 이유는 3단계 파이프라인의 사용에 있다.
* 사용할 CNN을 Train해야한다. 이때는 주로 이미 알려진 Network 중 성능이 좋은 모델의 가중치를 가져와 fine-tuning한다.
* fine-tune된 CNN에 맞게 SVM을 fitting 해야한다.
* Bounding Box Regression를 학습시켜야한다.

이 3개의 단계가 순차적으로 일어나기 때문에 학습시간이 비교적 오래걸릴 수 밖에 없다. 또한 힘들게 train시킨 R-CNN을 사용하여 Object Detection을 할 때에도 느린것은 마찬가지다. Input image에서 수많은 Object 후보들을 뽑아내고 이들을 연산량이 많은 CNN에 모두 집어 넣기 때문이다. 게다가, 이미지에서 추출한 Object 후보가 직사각형인 경우, CNN이 요구하는 input size로(일반적으로 정사각형) 이미지를 warp해야 하기 때문에 이 과정에서 이미지의 왜곡이 일어나 학습결과에 부정적인 영향을 끼칠 수 있다는 것도 R-CNN의 단점이다. 이 단점들은 나쁘지 않은 성능을 발휘하는 R-CNN임임에도 실제로 잘 사용되지 못하게 만들었고, 사용자로 하여금 다른 방법론을 강구하도록 만들었다. 



Q : 왜 SVM을 썻는가?
A : 성능이 더 좋았기 때문에


***


# SPP-Net (Spatial Pyramid Pooling)


나는 Multiple-Object Detection을 하려고 한다. R-CNN을 쓰려고 보니, Input image에서 Object 후보들을 찾는 것 까지는 좋은데, 이들을 Conv layer의 Input size로 Warp할때 정보왜곡현상이 생긴다는 점이 마음에 들지 않았다. 그런데 잘 생각해보니 Input image를 굳이 먼저 자르지 않더라도 Conv layer에서 이미지를 여러개의 filter로 잘라서 Feature Extracting을 수행한다. 즉, Conv layer에서 Cropping과 Warping의 기능을 할 수 있을 것 같다. 그래서 다음과 같이 순서를 바꿔보았다.


전체 Input image를 Conv layer에 넣고 Feature를 추출한 후, 검출된 Feature들을 SPP-layer에 입력한다. 그리고 그 결과를 Fully-Connected layer의 입력으로 사용한다.

SPP-Net에서는 이러한 구조적 순서 변화를 통해 인위적인 image crop이나 warp을 통한 정보손실을 막을 수 있었고, R-CNN과 달리 CNN 연산이 한번만 요구되므로 속도도 훨씬 빨라질 수 있었다.


***


# Fast R-CNN

  Fast R-CNN에서는 SPP-Net을 사용하다가 'bin의 갯수와 Model의 성능이 크게 상관없다'라는 경험적 데이터에 근거하여 하나의 bin만을 사용한 것이다. 이 방법으로 SPP-Net의 3단계 파이프라인에서 오는 학습단계의 성능(시간)문제를 해결할 수 있었으며, SPP-Net에서 Conv Net을 학습시키지 못했던 문제또한 개선하였다.(SVM->Softmax) 이 작다면 작을 수 있는 변화가 모델의 큰 변화를 주도한 셈이다. (SPP-Net에서 사용한 SPP를 single-level로 사용)

"ROI Pooling" Layer
  Conv. Layer을 통해서 나오 Feature Map에서 ROI 영역을 Pooling 하여 (X,X)로 만든다.
  ex) 21x14 -> 3x2 max pooling with stride (3,2) -> Output 7x7
  ex) 35x42 -> 5x6 max pooling with stride (5,6) -> Output 7x7

1. Takes an input and a set of bounding boxes. ex) 1000, 2000 개
2. Generate convolutional feature maps.
3. For each bbox, get a fixed-length feature verctor form ROI pooling layer
4. Outputs have two information
  K+1 class labels
  Bounding box locations

문제점 : Test-time 중 Selective Search 시간이 오래 걸린다. 2초. 기존 R-CNN에서는 큰 비중이 아니었지만, 속도가 빨라진 현재 Fast R-CNN에서는 큰 비중을 차지한다. Selective Search 는 GPU가 아닌 CPU에서 돈다.


***

# Faster R-CNN
## (RPN(Region Proposal Network) + Fast R-CNN)

Faster-CNN 의 궁극적인 목적은 Resion proposal network 와 Fast R-CNN object detection network 간의 computation 을 share 하는것임.

RPN 을 training 한 후 이를 이용하여 Fast R-CNN 을 training 한다. 이후 Fast R-CNN 에 의해 tuned 된 network 는 RPN 을 intialize 하는대에 이용되는 방식으로 반복된다.

RPN 
* k 개 anchor boxes ex) 9개
* 2k(존재한다/안한다) score classification layer
* 4k(x, y, width, height) coordinates regression layer
* FCN (Fully Convolution Network)

특정 anchor에 positive label이 할당되는 데에는 다음과 같은 기준이 있다.
* 가장 높은 Intersection-over-Union(IoU)을 가지고 있는 anchor.
* IoU > 0.7 을 만족하는 anchor.

비교결과
Fast R-CNN : 198 ms
Faster R-CNN : 59 ms

1. train RPN with ImageNet pre-trained model
2. train fast R-CNN using proposal generated by 1. [no params. sharing]
3. use conv trained by 2. to initialize model, fix shared conv, update RPN
4. fix shared conv, fine-tune fc

R-CNN 같이 region-based detectors 에 사용되는 convolution feature maps 를 이용하여 RPN 을 구성할 수 있다.

기존의 RPN 방식은 [아래 그림 a,b]과 같이 scale이나 filter size 를 multiple 하게 pyramid 형식으로 구성하였는데, 이 논문에서는 [그림 c]와 같이 regression reference 들을 pyramid 형식으로 구성함으로써 single-scale image를 사용하여 train, test 를 가능하도록 했고 이로인해 running speed 의 향상을 이루어냈다.

문제점
ROI 크기 때문에 손실되는 data

***


# Mask R-CNN

Faster R-CNN 와 마찬가지로 영상에서 여러 ROI 후보를 제안한다.
제안된 ROI 후보는 곧 경계 박스이며 해당 위치의 특징맵을 RoIAlign 방식으로 추출한다.
추출된 특징맵으로부터 오브젝트의 클래스를 분류함과 동시에 오브젝트의 마스크를 얻는다


원본 영상의 좌상단의 15x15 영역이 RoI (= 경계박스)다. 
CNN은 원본 이미지 128x128 을 입력 받아 25 x 25 의 특징맵을 출력한다.
원본 영상의 15 x 15 에 해당하는 특징맵의 크기는 2.93 x 2.93 이다.
128 x 128 이 25 x 25 로 작아졌다. 128 / 5.12 = 25 
15 / 5.12 = 2.93 이므로 원본 이미지의 15 x 15 는 최종 특징 맵의 2.93 x 2.93 이다.

RoIPool 은 이런 경우에 2.93을 반올림 하여 특징맵 좌상단의 3x3 부분만 가져와서(pool) 클래스를 예측하였다.
2.93은 반올림되어 3.00 이 된다.
0.07의 차이는 작아보일지 모르지만 이 차이는 최대 0.5 까지 발생하며, 이러한 정렬 불량 은 성능에 큰 영향을 미친다.
RoIAlign 은 2.93 x 2.93 에 해당하는 특징맵을 정렬(align)시켜 바이리니어 인터폴레이션을 사용하여 보정된 특징값을 사용한다.

RoIPool 과 RoIWarp는 align 에 대한 고려가 없이 반올림을 이용해서 특징맵의 RoI 영역을 가져온다.
RoIWarp 는 반올림을 이용해 특징맵의 RoI를 가져온 후에 bilinear 인터폴레이션을 이용해서 지정된 크기로 resize 한다.
그렇기 때문에 정렬 상태 불량(misalignment) 현상이 발생한다.
RoiAlign 은 반올림 등을 사용하지 않고 bilinear 인터폴레이션을 이용해서 특징맵의 RoI 영역을 정확하게 정렬되도록 한다.

그리고 올해 초 페이스북 AI 팀이 분할된 이미지를 마스킹하는 Mask R-CNN을 내놓았습니다. Faster R-CNN에 각 픽셀이 오브젝트에 해당하는 것인지 아닌지를 마스킹하는 네트워크(CNN)를 추가한 것입니다. 이를 바이너리 마스크binary mask라고 합니다. 페이스북 팀은 정확한 픽셀 위치를 추출하기 위해 CNN을 통과하면서 RoIPool 영역의 위치에 생기는 소숫점 오차를 2D 선형보간법bilinear interpolation을 통해 감소시켰다고 합니다. 이를 RoIAlign이라고 합니다.

> Interpolation(인터폴레이션, 보간)이란 알려진 지점의 값 사이(중간)에 위치한 값을 알려진 값으로부터 추정하는 것을 말한다.

RoIAlign
* Avoid any quantization, realizing alignment between mask and instance(e.g. use x/16x/16 instead of [x/16][x/16])
* Use bilinear interpolation on feature map
* Compared to RoIWarp, which also adopts bilinear resampling, proving the effectiveness of RoIAlign mainly comes from alignment instead of bilinear interpolation.
